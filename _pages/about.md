---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi! I am Shengjie Luo, a third-year PhD student at School of Intelligence Science and Technology in Peking University, advised by [Prof.Liwei Wang](http://www.liweiwang-pku.com/) and [Prof.Di He](https://dihe-pku.github.io/). Before that, I finished my undergraduate study at ShenYuan Honors College in Beihang University, majoring in Computer Science.

My main research area lies in machine learning, with special interests in models and algorithms inspired by theoretical insights. Recently, I am focusing on designing expressive, efficient and effective Transformers and Graph Neural Networks with applications in Graph Learning, AI for Science and Natural Language Processing. I have published several papers and been reviewers at the top machine learning and artificial intelligence conferences such as NeurIPS, ICML and ICLR.

If you are interested in collaborating with me or want to have a chat, always feel free to contact me through e-mail or WeChat :)



# üî• News
- *2022.09*: One paper is accepted at NeurIPS 2022! 
- *2021.09*: Two papers are accepted at NeurIPS 2021!
- *2021.06*: Graphormer won the 1st place in [PCQM4M Track, OGB Large-Scale Challenge, KDD CUP 2021](https://ogb.stanford.edu/kddcup2021/results/#awardees-of-pcqm4m-lsc-track-leaderboard)!
- *2021.05*: One paper is accepted at ICML 2021!

# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><img src='images/URPE.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**NeurIPS 2022**] [Your Transformer May Not be as Powerful as You Expect](https://arxiv.org/abs/2205.13401)

**Shengjie Luo**\*, Shanda Li\*, Shuxin Zheng, Tie-Yan Liu, Liwei Wang

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ImWO7WYAAAAJ&citation_for_view=ImWO7WYAAAAJ:_FxGoFyzp5QC)
- TBA. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/Graphormer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**NeurIPS 2021**] [Do Transformers Really Perform Badly for Graph Representation?](https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html)

Chengxuan Ying, Tianle Cai, **Shengjie Luo**, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ImWO7WYAAAAJ&citation_for_view=ImWO7WYAAAAJ:zYLM7Y9cAGgC)<strong><span class='show_paper_citations' data='ImWO7WYAAAAJ:_FxGoFyzp5QC'></span></strong>
- TBA. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/kernel_rpe.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**NeurIPS 2021**] [Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding](https://proceedings.neurips.cc/paper/2021/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html)

**Shengjie Luo**\*, Shanda Li\*, Tianle Cai, Dinglan Peng, Di He, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ImWO7WYAAAAJ&citation_for_view=ImWO7WYAAAAJ:UeHWp8X0CEIC)
- TBA. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/graphnorm.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**ICML 2021**] [GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training](https://proceedings.mlr.press/v139/cai21e)

Tianle Cai\*, **Shengjie Luo**\*, Keyulu Xu, Di He, Tie-Yan Liu, Liwei Wang

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=ImWO7WYAAAAJ&citation_for_view=ImWO7WYAAAAJ:IjCSPb-OGe4C)
- TBA. 
</div>
</div>

# üéñ Honors and Awards
- *2021.06* 1st place Winner of PCQM4M Track, OGB Large Scale Challenge, KDD CUP 2021.
- *2018.12* National Scholarship (Top 1%). 

# üìñ Educations
- *2022.09 - 2025.07 (expected)*, PhD Student, School of Intelligence Science and Technology, Peking University.
- *2020.09 - 2022.07*,            Master Student, Academy for Advanced Interdisciplinary Studies, Peking University. 
- *2016.09 - 2020.07*,            Undergraduate Student, Shenyuan Honors College, Beihang University. 

# üí¨ Invited Talks
- *2022.03*, Huawei Technologies Noah's Ark Lab. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding.
- *2022.02*, AI Time. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. \| [\[media\]](https://mp.weixin.qq.com/s/EjPEwcvbl0xWAmjeVbPpNg) 

# üíª Internships
- *2021.12 - now*,     Machine Learning Group, Microsoft Research Asia, China.
- *2020.10 - 2021.06*, Machine Learning Group, Microsoft Research Asia, China.
- *2019.10 - 2020.06*, Natural Language Computing Group, Microsoft Research Asia, China.

# üè´ Professional Services
- Reviewer for ICLR 2022, NeurIPS 2022, LOG 2022.
